# Home_Sales

In this challenge,  SparkSQL is used to determine key metrics about home sales data such as to create temporary views, partition the data, cache and uncache a temporary table, and verifying that the table has been uncached.

A new repository has been created with the name Home_Sales and it has the file Home_Sales_colab.ipynb. All the quries has been answered using SparkSQL and Time has been noted for the asked query for  cached and uncached home sales temporary table. 

Partitioning has been done on date_built field using Parque on home sales data. Asked query has been run and also noted the runtime.

Everytime the runtime calculated was different for cached and uncached data.

home_sales table was uncached and verified uncached by using PySpark.

Home_Sales_Colab.ipynb has been downloaded from google colab and uploaded to Github Repository.
